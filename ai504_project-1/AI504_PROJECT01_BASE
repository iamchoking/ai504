{"cells":[{"cell_type":"markdown","metadata":{"id":"natHkvbtkuaD"},"source":["# Submission\n","Main code (```20244512.py```)\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DT74Ht9Tcaxs","executionInfo":{"status":"ok","timestamp":1729639679978,"user_tz":-540,"elapsed":411328,"user":{"displayName":"Hyungho Chris Choi","userId":"06411252990383916682"}},"outputId":"4dc072a0-59e1-442b-c87c-252c0aea49af"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 26421880/26421880 [00:00<00:00, 49813660.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 29515/29515 [00:00<00:00, 23761013.93it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["\n","100%|██████████| 4422102/4422102 [00:00<00:00, 12394228.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 5148/5148 [00:00<00:00, 6880904.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n","\n","Epoch [1/20], Loss: 0.3892, LR: 0.001000\n","Epoch [2/20], Loss: 0.2546, LR: 0.001000\n","Epoch [3/20], Loss: 0.2134, LR: 0.001000\n","Epoch [4/20], Loss: 0.1836, LR: 0.001000\n","Epoch [5/20], Loss: 0.1600, LR: 0.001000\n","Epoch [6/20], Loss: 0.1375, LR: 0.001000\n","Epoch [7/20], Loss: 0.1207, LR: 0.001000\n","Epoch [8/20], Loss: 0.1052, LR: 0.001000\n","Epoch [9/20], Loss: 0.0909, LR: 0.001000\n","Epoch [10/20], Loss: 0.0799, LR: 0.000100\n","Epoch [11/20], Loss: 0.0375, LR: 0.000100\n","Epoch [12/20], Loss: 0.0230, LR: 0.000100\n","Epoch [13/20], Loss: 0.0161, LR: 0.000100\n","Epoch [14/20], Loss: 0.0114, LR: 0.000100\n","Epoch [15/20], Loss: 0.0080, LR: 0.000100\n","Epoch [16/20], Loss: 0.0062, LR: 0.000100\n","Epoch [17/20], Loss: 0.0040, LR: 0.000100\n","Epoch [18/20], Loss: 0.0036, LR: 0.000100\n","Epoch [19/20], Loss: 0.0025, LR: 0.000100\n","Epoch [20/20], Loss: 0.0021, LR: 0.000010\n","Accuracy: 92.79%\n"]}],"source":["import torch\n","import random\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","from torchvision import datasets\n","from torch.utils.data import DataLoader\n","import numpy as np\n","\n","def set_seed(seed=0):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","# Define the CNN model\n","class AI504model01(nn.Module):\n","    def __init__(self):\n","        super(AI504model01, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n","        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n","        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n","\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.batch_norm1 = nn.BatchNorm2d(32)\n","        self.batch_norm2 = nn.BatchNorm2d(64)\n","        self.batch_norm3 = nn.BatchNorm2d(128)\n","        self.batch_norm4 = nn.BatchNorm2d(256)\n","\n","        # Fully connected layers\n","        self.fc1 = nn.Linear(256 * 1 * 1, 512)  # Adjusted to the correct size after pooling\n","        self.fc2 = nn.Linear(512, 10)\n","        self.dropout = nn.Dropout(0.5)\n","\n","    def forward(self, x):\n","        x = self.pool(torch.relu(self.batch_norm1(self.conv1(x))))\n","        x = self.pool(torch.relu(self.batch_norm2(self.conv2(x))))\n","        x = self.pool(torch.relu(self.batch_norm3(self.conv3(x))))\n","        x = self.pool(torch.relu(self.batch_norm4(self.conv4(x))))\n","\n","        # Flatten the tensor dynamically\n","        x = torch.flatten(x, 1)\n","\n","        x = torch.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","\n","        return x\n","\n","# Load the Fashion-MNIST dataset\n","def load_data(batch_size=64):\n","    transform = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5,), (0.5,))\n","    ])\n","\n","    train_set = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n","    test_set = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n","\n","    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True )\n","    test_loader  = DataLoader(test_set , batch_size=batch_size, shuffle=False)\n","\n","    return train_loader, test_loader\n","\n","# Training the model\n","def train_model(model, train_loader, criterion, optimizer, scheduler, device, epochs=20):\n","    model.train()\n","    for epoch in range(epochs):\n","        running_loss = 0.0\n","        for images, labels in train_loader:\n","            images, labels = images.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","        print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}, LR: {scheduler.get_last_lr()[0]:.6f}')\n","        # Step the scheduler to adjust the learning rate\n","        scheduler.step()\n","\n","## Save / Evaluation tools\n","\n","# Generate logits for the test set\n","def generate_logits(model, test_loader, device):\n","    model.eval()\n","    logits_list = []\n","    with torch.no_grad():\n","        for images, _ in test_loader:\n","            images = images.to(device)\n","            outputs = model(images)\n","            logits_list.append(outputs.cpu().numpy())\n","\n","    logits = np.vstack(logits_list)\n","    return logits\n","\n","# Save the logits to a .npy file\n","def save_logits(logits, file_name):\n","    np.save(file_name, logits)\n","\n","# calculate accuracy in runtime\n","def calc_accuracy(model, test_loader):\n","    model.eval()\n","    correct = 0.0\n","    for x, y in test_loader:\n","        x, y = x.to(device), y.to(device)\n","        out = model(x)\n","        correct += (out.argmax(1) == y).float().sum().item()\n","    print(f'Accuracy: {100. * correct / len(test_loader.dataset):.2f}%')\n","\n","## Main Script\n","\n","set_seed()\n","\n","student_id = \"20244512\"\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load data\n","train_loader, test_loader = load_data(batch_size=64)\n","\n","# Initialize model, loss function, and optimizer\n","model = AI504model01().to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Add learning rate scheduler (StepLR reduces LR by gamma every step_size epochs)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","\n","# Train the model (with scheduler)\n","train_model(model, train_loader, criterion, optimizer, scheduler, device, epochs=20)\n","\n","# Generate logits for the test set\n","logits = generate_logits(model, test_loader, device)\n","\n","# Save the logits as a .npy file\n","save_logits(logits, f\"{student_id}.npy\")\n","\n","# Calculate accuracy of the final model\n","calc_accuracy(model, test_loader)"]},{"cell_type":"markdown","metadata":{"id":"b6mI4vG1lwrN"},"source":["# Accessories"]},{"cell_type":"markdown","metadata":{"id":"3c8RQV6ElyoJ"},"source":["Additional Training"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"AziCV5XIi3qF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729640677788,"user_tz":-540,"elapsed":194570,"user":{"displayName":"Hyungho Chris Choi","userId":"06411252990383916682"}},"outputId":"47cb6120-71c7-4346-a296-707187afa171"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/10], Loss: 0.0011, LR: 0.000010\n","Epoch [2/10], Loss: 0.0009, LR: 0.000010\n","Epoch [3/10], Loss: 0.0008, LR: 0.000010\n","Epoch [4/10], Loss: 0.0007, LR: 0.000010\n","Epoch [5/10], Loss: 0.0005, LR: 0.000010\n","Epoch [6/10], Loss: 0.0005, LR: 0.000010\n","Epoch [7/10], Loss: 0.0005, LR: 0.000010\n","Epoch [8/10], Loss: 0.0004, LR: 0.000010\n","Epoch [9/10], Loss: 0.0004, LR: 0.000010\n","Epoch [10/10], Loss: 0.0004, LR: 0.000001\n","Accuracy: 92.95%\n"]}],"source":["# train 10 more epochs, save and calculate accuracy\n","\n","# Train the model with scheduler\n","train_model(model, train_loader, criterion, optimizer, scheduler, device, epochs=10)\n","# Generate logits for the test set\n","logits = generate_logits(model, test_loader, device)\n","# Save the logits as a .npy file\n","save_logits(logits, f\"{student_id}.npy\")\n","# Calculate accuracy of the final model\n","calc_accuracy(model, test_loader)"]},{"cell_type":"markdown","metadata":{"id":"gpnwWGb5l10M"},"source":["Simple logit testing"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2483,"status":"ok","timestamp":1729640690919,"user":{"displayName":"Hyungho Chris Choi","userId":"06411252990383916682"},"user_tz":-540},"id":"Q8tMMIXLdgmT","outputId":"e9385b48-463e-49af-9144-c1fb4c2b593e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["92.95"]},"metadata":{},"execution_count":7}],"source":["targets = torch.cat([target for _, target in test_loader]).cpu()\n","logits = np.load(\"./20244512.npy\")\n","\n","def calculate_accuracy_from_logits(logits, targets):\n","    pred = np.argmax(logits, axis=1)\n","    accuracy = 100. * np.sum(pred == targets.numpy()) / len(targets)\n","    return round(accuracy, 2)\n","\n","calculate_accuracy_from_logits(logits,targets)"]},{"cell_type":"markdown","metadata":{"id":"qnr5F-nll5yX"},"source":["## Full test (```test.py```)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2637,"status":"ok","timestamp":1729640695508,"user":{"displayName":"Hyungho Chris Choi","userId":"06411252990383916682"},"user_tz":-540},"id":"rBv-fWgSe5BP","outputId":"fd8399b6-7c57-4bd1-bae3-04f9d5d4a623"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Test dataset size: 10000\n","20244512 - Accuracy: 92.95%\n"]}],"source":["import os\n","import re\n","import random\n","import numpy as np\n","import torch\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","\n","# Function to set random seed\n","def set_seed(seed=0):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","# Function to prepare test data\n","def prepare_test_data(batch_size=64):\n","    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n","    test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n","    return DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Function to calculate accuracy\n","def calculate_accuracy(logits, targets):\n","    pred = np.argmax(logits, axis=1)\n","    accuracy = 100. * np.sum(pred == targets.numpy()) / len(targets)\n","    return round(accuracy, 2)\n","\n","# Main function to evaluate logits\n","def evaluate(logits_filename):\n","    set_seed()\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    print(f\"Using device: {device}\")\n","\n","    test_loader = prepare_test_data()\n","    print(f'Test dataset size: {len(test_loader.dataset)}')\n","\n","    try:\n","        # Ensure file name is valid\n","        assert re.match(r\"\\d{8}\\.npy\", logits_filename), \"File name must be an 8-digit student ID followed by '.npy'.\"\n","\n","        # Load logits\n","        logits = np.load(logits_filename)\n","        assert logits.shape == (len(test_loader.dataset), 10), f\"Logits shape mismatch: expected ({len(test_loader.dataset)}, 10).\"\n","\n","        # Calculate accuracy\n","        targets = torch.cat([target for _, target in test_loader]).cpu()\n","        accuracy = calculate_accuracy(logits, targets)\n","\n","    except AssertionError as e:\n","        accuracy = 0.0\n","        print(f\"Evaluation failed: {e}\")\n","\n","    print(f'{logits_filename[:-4]} - Accuracy: {accuracy:.2f}%')\n","\n","if __name__ == \"__main__\":\n","    evaluate(\"20244512.npy\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"19W-CWDM3DdrpKOkVZn9X7K5XX9PL6jvj","timestamp":1729572752554}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":0}